                 Procedure for remove a master node from kubernetes cluster.
                 **********************************************************

 I am going to remove 192.168.56.142(kubemas3.anbu.com) master node from kubernetes cluster.
 
    List the etcd members
    ---------------------
 
[root@kubemas1 kubelet]# etcdctl member list

9f04057cd10cb0b7, started, 192.168.56.142, https://192.168.56.142:2380, https://192.168.56.142:2379, false
a4ee350447b1c083, started, 192.168.56.144, https://192.168.56.144:2380, https://192.168.56.144:2379, false
afd5b034595cfe09, started, 192.168.56.143, https://192.168.56.143:2380, https://192.168.56.143:2379, false

   Stop & disable etcd service 
   
#systemctl stop etcd && systemctl disable etcd 

   Stop & disable kubelet service 
 
#systemctl stop kubelet && systemctl disable kubelet

   Drain the Node from cluster
   ---------------------------
 
[root@kubemas1 kubelet]# kubectl drain kubemas3.anbu.com
node/kubemas3.anbu.com already cordoned

   Remove a member from cluster
   ----------------------------

[root@kubemas1 kubelet]# etcdctl member remove 9f04057cd10cb0b7

Member 9f04057cd10cb0b7 removed from cluster be7714fdd77450dc

 Reset the node from 192.168.56.142(kubemas3.anbu.com) server.
 ------------------------------------------------------------
 
 [root@kubemas3 etcd]# kubeadm reset
 
[reset] WARNING: Changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
[reset] Are you sure you want to proceed? [y/N]: y
[preflight] Running pre-flight checks
W0118 18:26:58.230052    1793 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory
[reset] No etcd config found. Assuming external etcd
[reset] Please, manually reset etcd to prevent further issues
[reset] Stopping the kubelet service
[reset] Unmounting mounted directories in "/var/lib/kubelet"
[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
[reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]
[reset] Deleting contents of stateful directories: [/var/lib/kubelet /var/lib/dockershim /var/run/kubernetes /var/lib/cni]

The reset process does not clean CNI configuration. To do so, you must remove /etc/cni/net.d

The reset process does not reset or clean up iptables rules or IPVS tables.
If you wish to reset iptables, you must do so manually by using the "iptables" command.

If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
to reset your system's IPVS tables.

The reset process does not clean your kubeconfig files and you must remove them manually.
Please, check the contents of the $HOME/.kube/config file.


 

#kubectl -n kube-system edit cm kubeadm-config -oyaml

# Please edit the object below. Lines beginning with a '#' will be ignored,
# and an empty file will abort the edit. If an error occurs while saving this file will be
# reopened with the relevant failures.
#
apiVersion: v1
data:
  ClusterConfiguration: |
    apiServer:
      extraArgs:
        authorization-mode: Node,RBAC
      timeoutForControlPlane: 4m0s
    apiVersion: kubeadm.k8s.io/v1beta2
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controlPlaneEndpoint: 192.168.56.145:6443
    controllerManager: {}
    dns:
      type: CoreDNS
    etcd:
      external:
        caFile: /etc/etcd/ca.pem
        certFile: /etc/etcd/kubernetes.pem
        endpoints:
        - https://192.168.56.143:2379
        - https://192.168.56.144:2379
        - https://192.168.56.142:2379                        -  We should remove this line and exit
        keyFile: /etc/etcd/kubernetes-key.pem
    imageRepository: k8s.gcr.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.17.0
    networking:
      dnsDomain: cluster.local
      podSubnet: 10.30.0.0/24
      serviceSubnet: 10.96.0.0/12
    scheduler: {}
  ClusterStatus: |
    apiEndpoints:
      kubemas1.anbu.com:
        advertiseAddress: 10.0.2.15
        bindPort: 6443
      kubemas2.anbu.com:
        advertiseAddress: 10.0.2.15
        bindPort: 6443


 Delete the Node from cluster
 ----------------------------

[root@kubemas1 kubelet]# kubectl delete node kubemas3.anbu.com
node "kubemas3.anbu.com" deleted


